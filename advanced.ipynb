{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe68ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aprel\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "def feature_func(traj):\n",
    "    \"\"\"Returns the features of the given MountainCar trajectory, i.e. \\Phi(traj).\n",
    "    \n",
    "    Args:\n",
    "        traj: List of state-action tuples, e.g. [(state0, action0), (state1, action1), ...]\n",
    "    \n",
    "    Returns:\n",
    "        features: a numpy vector corresponding the features of the trajectory\n",
    "    \"\"\"\n",
    "    states = np.array([pair[0] for pair in traj])\n",
    "    actions = np.array([pair[1] for pair in traj[:-1]])\n",
    "    min_pos, max_pos = states[:,0].min(), states[:,0].max()\n",
    "    mean_speed = np.abs(states[:,1]).mean()\n",
    "    mean_vec = [-0.703, -0.344, 0.007]\n",
    "    std_vec = [0.075, 0.074, 0.003]\n",
    "    return (np.array([min_pos, max_pos, mean_speed]) - mean_vec) / std_vec\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Create the OpenAI Gym environment\n",
    "    gym_env = gym.make(args['env'])\n",
    "    \n",
    "    # Seed for reproducibility\n",
    "    np.random.seed(args['seed'])\n",
    "    gym_env.seed(args['seed'])\n",
    "\n",
    "    # Wrap the environment with a feature function\n",
    "    env = aprel.Environment(gym_env, args['feature_func'])\n",
    "\n",
    "    # Create a trajectory set\n",
    "    trajectory_set = aprel.generate_trajectories_randomly(env, num_trajectories=args['num_trajectories'],\n",
    "                                                          max_episode_length=args['max_episode_length'],\n",
    "                                                          file_name=args['env'],headless=args['headless'], seed=args['seed'])\n",
    "    features_dim = len(trajectory_set[0].features)\n",
    "\n",
    "    # Initialize the query optimizer\n",
    "    query_optimizer = aprel.QueryOptimizerDiscreteTrajectorySet(trajectory_set)\n",
    "\n",
    "    # Initialize the object for the true human\n",
    "    if args['simulate']:\n",
    "        true_params = {'weights': aprel.util_funs.get_random_normalized_vector(features_dim)}\n",
    "        true_user = aprel.SoftmaxUser(true_params)\n",
    "    else:\n",
    "        true_user = aprel.HumanUser(delay=args['human_visualization_delay'])\n",
    "    \n",
    "    # Create the human response model and initialize the belief distribution\n",
    "    params = {'weights': aprel.util_funs.get_random_normalized_vector(features_dim)}\n",
    "    user_model = aprel.SoftmaxUser(params)\n",
    "    belief = aprel.SamplingBasedBelief(user_model, [], params, logprior=args['log_prior_belief'],\n",
    "                                       num_samples=args['num_samples'],\n",
    "                                       proposal_distribution=args['proposal_distribution'],\n",
    "                                       burnin=args['burnin'], thin=args['thin'])\n",
    "    # Report the metrics\n",
    "    print('Estimated user parameters: ' + str(belief.mean))\n",
    "\n",
    "    # Initialize a dummy query so that the query optimizer will generate queries of the same kind\n",
    "    if args['query_type'] == 'preference':\n",
    "        query = aprel.PreferenceQuery(trajectory_set[:args['query_size']])\n",
    "    elif args['query_type'] == 'weak_comparison':\n",
    "        query = aprel.WeakComparisonQuery(trajectory_set[:args['query_size']])\n",
    "    elif args['query_type'] == 'full_ranking':\n",
    "        query = aprel.FullRankingQuery(trajectory_set[:args['query_size']])\n",
    "    else:\n",
    "        raise NotImplementedError('Unknown query type.')\n",
    "\n",
    "    # Active learning loop\n",
    "    for query_no in range(args['num_iterations']):\n",
    "        # Optimize the query\n",
    "        queries, objective_values = query_optimizer.optimize(args['acquisition'], belief,\n",
    "                                                             query, batch_size=args['batch_size'], \n",
    "                                                             optimization_method=args['optim_method'],\n",
    "                                                             reduced_size=args['reduced_size_for_batches'],\n",
    "                                                             gamma=args['dpp_gamma'],\n",
    "                                                             distance=args['distance_metric_for_batches'])\n",
    "        print('Objective Values: ' + str(objective_values))\n",
    "\n",
    "        # Ask the query to the human\n",
    "        responses = true_user.respond(queries)\n",
    "        \n",
    "        #Update the belief distribution\n",
    "        belief.update([aprel.Preference(query, response) for query, response in zip(queries, responses)])\n",
    "        \n",
    "        # Report the metrics\n",
    "        print('Estimated user parameters: ' + str(belief.mean))\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env', type=str, required=True,\n",
    "                        help='The name of the OpenAI Gym environment.')\n",
    "    parser.add_argument('--seed', type=int, default=0,\n",
    "                        help='Seed for numpy randomness.')\n",
    "    parser.add_argument('--num_trajectories', type=int, default=40,\n",
    "                        help='Number of trajectories in the discrete trajectory set for query optimization.')\n",
    "    parser.add_argument('--max_episode_length', type=int, default=None,\n",
    "                        help='Maximum number of time steps per episode ONLY FOR the new trajectories. Defaults to no limit.')\n",
    "    parser.add_argument('--restore', dest='restore', action='store_true',\n",
    "                        help='Use this flag if you want to restore the discrete trajectory set from the existing data folder.')\n",
    "    parser.set_defaults(restore=False)\n",
    "    parser.add_argument('--headless', dest='headless', action='store_true',\n",
    "                        help='Use this flag if you want to run the code in a headless way, i.e., with no visualization.')\n",
    "    parser.set_defaults(headless=False)\n",
    "    parser.add_argument('--simulate', dest='simulate', action='store_true',\n",
    "                        help='Use this flag if you want to run the code with simulated synthetic users who follow a softmax model.')\n",
    "    parser.set_defaults(simulate=False)\n",
    "    parser.add_argument('--human_visualization_delay', type=float, default=0.5,\n",
    "                        help='Delay between each trajectory visualization during querying.')\n",
    "    parser.add_argument('--num_samples', type=int, default=100,\n",
    "                        help='Number of samples for the sampling based belief.')\n",
    "    parser.add_argument('--burnin', type=int, default=200,\n",
    "                        help='Number of burn-in steps for Metropolis-Hastings in the sampling based belief.')\n",
    "    parser.add_argument('--thin', type=int, default=20,\n",
    "                        help='Thinning parameter for Metropolis-Hastings in the sampling based belief.')\n",
    "    parser.add_argument('--query_type', type=str, default='preference',\n",
    "                        help='Type of the queries that will be actively asked to the user. Options: preference, weak_comparison, full_ranking.')\n",
    "    parser.add_argument('--query_size', type=int, default=2,\n",
    "                        help='Number of trajectories in each query.')\n",
    "    parser.add_argument('--num_iterations', type=int, default=10,\n",
    "                        help='Number of iterations in the active learning loop.')\n",
    "    parser.add_argument('--optim_method', type=str, default='exhaustive_search',\n",
    "                        help='Options: exhaustive_search, greedy, medoids, boundary_medoids, successive_elimination, dpp.')\n",
    "    parser.add_argument('--batch_size', type=int, default=1,\n",
    "                        help='Batch size can be set >1 for batch active learning algorithms.')\n",
    "    parser.add_argument('--acquisition', type=str, default='random',\n",
    "                        help='Acquisition function for active querying. Options: mutual_information, volume_removal, disagreement, regret, random, thompson')\n",
    "    parser.add_argument('--reduced_size_for_batches', type=int, default=100,\n",
    "                        help='The number of greedily chosen candidate queries (reduced set) for batch generation.')\n",
    "    parser.add_argument('--dpp_gamma', type=int, default=1,\n",
    "                        help='Gamma parameter for the DPP method: the higher gamma the more important is the acquisition function relative to diversity.')\n",
    "\n",
    "    args = vars(parser.parse_args())\n",
    "    args['feature_func'] = feature_func\n",
    "    args['log_prior_belief'] = aprel.uniform_logprior\n",
    "    args['proposal_distribution'] = aprel.gaussian_proposal\n",
    "    args['distance_metric_for_batches'] = aprel.default_query_distance # all relevant methods default to default_query_distance\n",
    "\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
